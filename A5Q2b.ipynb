{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A5Q2b.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DD--Bo2gU1qr","colab_type":"text"},"source":["# Load metadata\n","This assumes that the file train.zip has been unzipped in the current directory. If needed, insert code here to load the data from your computer."]},{"cell_type":"code","metadata":{"id":"tl2fPH9mU2ao","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"dfb710a4-9287-4550-b653-fef8a15fa63a","executionInfo":{"status":"ok","timestamp":1564375442357,"user_tz":240,"elapsed":1823,"user":{"displayName":"Ming Huang","photoUrl":"https://lh5.googleusercontent.com/-zrotzI3mEl0/AAAAAAAAAAI/AAAAAAAAABU/J_3s5_fwd8U/s64/photo.jpg","userId":"07501724645488894229"}}},"source":["import json\n","import numpy as np\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","# !unzip \"/content/drive/My Drive/Colab Notebooks/train.zip\"\n","# !unzip \"/content/drive/My Drive/Colab Notebooks/fr-en.zip\"\n","!# load metadata\n","with open(\"train.json\", 'r') as f:\n","    metadata = json.load(f)\n","n_claims = len(metadata)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p8cv-JHLWNWb","colab_type":"text"},"source":["# Preprocess articles\n","\n","This code preprocesses the aticles to extract the top 5 sentences with greatest similarity to the claim according to tf-idf."]},{"cell_type":"code","metadata":{"id":"lgWYI7OyWMbb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"outputId":"c66d325e-7c31-49cf-8753-a32749f106d3","executionInfo":{"status":"ok","timestamp":1564375443448,"user_tz":240,"elapsed":2905,"user":{"displayName":"Ming Huang","photoUrl":"https://lh5.googleusercontent.com/-zrotzI3mEl0/AAAAAAAAAAI/AAAAAAAAABU/J_3s5_fwd8U/s64/photo.jpg","userId":"07501724645488894229"}}},"source":["def preprocess_articles():\n","\n","    from nltk.tokenize import sent_tokenize\n","    import nltk\n","    nltk.download('punkt')\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    \n","    # load metadata\n","    with open(\"train.json\", 'r') as f:\n","        metadata = json.load(f)\n","    n_claims = 1#len(metadata)\n","\n","    # load related articles for each claim\n","    relevant_sentences = []\n","    for id in range(n_claims):\n","  \n","        if id % 500 == 0:\n","            print(\"Claims preprocessed: \",id)\n","        \n","        # retrieve related articles\n","        related_articles = metadata[id]['related_articles']\n","        articles = \"\"\n","        for article_id in related_articles:\n","            filename = \"train_articles/\" + str(article_id) + \".txt\"\n","            # concatenate related articles\n","            with open(filename, 'r') as text_file:\n","                text = text_file.read()\n","                articles = articles + \"\\n\" + text\n","\n","        # split articles into sentences\n","        sentences = sent_tokenize(articles)\n","\n","        # append claim to articles\n","        sentences.append(metadata[id]['claim'])\n","\n","        # vectorize sentences based on tf-idf\n","        vectorizer = TfidfVectorizer()\n","        X = vectorizer.fit_transform(sentences)\n","    \n","        # measure similarity between claim and each sentence\n","        similarity =  X[-1,:] @ np.transpose(X[:-2,:])\n","        similarity = similarity.todense()\n","\n","        # find top 5 sentences with greatest similarity\n","        sorted_index = np.argsort(similarity)\n","        top_sentences = []\n","        for i in range(1,min(5,sorted_index.shape[1])+1):\n","            top_sentences.append(sentences[sorted_index[0,-i]])\n","        relevant_sentences.append(top_sentences)\n","\n","   \n","    return metadata, relevant_sentences\n","\n","metadata, relevant_sentences = preprocess_articles()\n","print(\"Metadata of claim 0:\")\n","print(metadata[0]['claim'])\n","print(\"Metadata of claimant 0:\")\n","print(metadata[0]['claimant'])\n","print(\"Relevant sentences of claim 0:\")\n","print(relevant_sentences[0])\n","# print(relevant_sentences[0][0])\n","# print(relevant_sentences[0][1])\n","# print(relevant_sentences[0][2])\n","# print(relevant_sentences[0][3])\n","# print(relevant_sentences[0][4])\n"],"execution_count":28,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","Claims preprocessed:  0\n","Metadata of claim 0:\n","A line from George Orwell's novel 1984 predicts the power of smartphones.\n","Metadata of claimant 0:\n","\n","Relevant sentences of claim 0:\n","['1984 by George Orwell\\n1984 is a dystopian novel by English author George Orwell published in 1949.', 'Theater Review: \\'1984\\'\\nEarly this year, sales of George Orwell\\'s novel \"1984\" spiked after the words \"alternative facts\" entered the lexicon.', 'It is truly frightening to see the parallels between George Orwell\\'s dystopian novel \"1984\" and the state of our union today.', '\\n1984: George Orwell predicted 2017 almost 70 years ago\\nApril, 1984.', 'The line is from one of the characters that works for the Government, otherwise known as Big Brother.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"itgrdrSXWDdc","colab_type":"text"},"source":["# Sample a claim\n","Sample a claim, tokenize it and embed it."]},{"cell_type":"code","metadata":{"id":"AO2kG5PmV-ak","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":575},"outputId":"1758ab50-eeac-42a4-b9aa-5f98a1a66409","executionInfo":{"status":"ok","timestamp":1564375450749,"user_tz":240,"elapsed":10195,"user":{"displayName":"Ming Huang","photoUrl":"https://lh5.googleusercontent.com/-zrotzI3mEl0/AAAAAAAAAAI/AAAAAAAAABU/J_3s5_fwd8U/s64/photo.jpg","userId":"07501724645488894229"}}},"source":["!pip install bpemb\n","from bpemb import BPEmb\n","n_embedding_dims = 50\n","bpemb_en = BPEmb(lang=\"en\", dim=n_embedding_dims)\n","\n","def sampleClaim(metadata): \n","#     id = random.randint(0, len(metadata) - 1)\n","    id = random.randint(0, len(relevant_sentences) - 1)\n","    claim = metadata[id][\"claim\"]\n","    claimant = metadata[id][\"claimant\"]\n","    label = metadata[id][\"label\"]\n","    \n","    if (len(relevant_sentences[id]) != 5): return \"SKIP\", \"SKIP\", \"SKIP\", \"SKIP\", \"SKIP\", \"SKIP\", \"SKIP\", \"SKIP\", \"SKIP\", \"SKIP\"\n","\n","    sentence_0 = relevant_sentences[id][0]\n","    sentence_1 = relevant_sentences[id][1]\n","    sentence_2 = relevant_sentences[id][2]\n","    sentence_3 = relevant_sentences[id][3]\n","    sentence_4 = relevant_sentences[id][4]\n","    \n","        \n","    embedding_claim = bpemb_en.embed(claim)\n","    embedding_claimant = bpemb_en.embed(claimant)\n","    embedded_sentence_0 = bpemb_en.embed(sentence_0)\n","    embedded_sentence_1 = bpemb_en.embed(sentence_1)\n","    embedded_sentence_2 = bpemb_en.embed(sentence_2)\n","    embedded_sentence_3 = bpemb_en.embed(sentence_3)\n","    embedded_sentence_4 = bpemb_en.embed(sentence_4)\n","    \n","    \n","    embedding_claim = np.reshape(embedding_claim,(embedding_claim.shape[0],1,embedding_claim.shape[1]))\n","    embedding_claimant = np.reshape(embedding_claimant,(embedding_claimant.shape[0],1,embedding_claimant.shape[1]))\n","    embedded_sentence_0 = np.reshape(embedded_sentence_0,(embedded_sentence_0.shape[0],1,embedded_sentence_0.shape[1]))\n","    embedded_sentence_1 = np.reshape(embedded_sentence_1,(embedded_sentence_1.shape[0],1,embedded_sentence_1.shape[1]))\n","    embedded_sentence_2 = np.reshape(embedded_sentence_2,(embedded_sentence_2.shape[0],1,embedded_sentence_2.shape[1]))\n","    embedded_sentence_3 = np.reshape(embedded_sentence_3,(embedded_sentence_3.shape[0],1,embedded_sentence_3.shape[1]))\n","    embedded_sentence_4 = np.reshape(embedded_sentence_4,(embedded_sentence_4.shape[0],1,embedded_sentence_4.shape[1]))\n","    \n","    label = metadata[id][\"label\"]\n","    label_tensor = torch.tensor([label], dtype=torch.long)\n","    \n","    claim_tensor = torch.tensor(embedding_claim, dtype=torch.float)\n","    claimant_tensor = torch.tensor(embedding_claimant, dtype=torch.float)\n","    embedded_sentence_0 = torch.tensor(embedded_sentence_0, dtype=torch.float)\n","    embedded_sentence_1 = torch.tensor(embedded_sentence_1, dtype=torch.float)\n","    embedded_sentence_2 = torch.tensor(embedded_sentence_2, dtype=torch.float)\n","    embedded_sentence_3 = torch.tensor(embedded_sentence_3, dtype=torch.float)\n","    embedded_sentence_4 = torch.tensor(embedded_sentence_4, dtype=torch.float)\n","    \n","    claim_claimant_tensor = torch.cat((claim_tensor, claimant_tensor), 0)\n","    \n","    claim_claimant_sentences_tensor = torch.cat((claim_tensor, claimant_tensor, embedded_sentence_0,\n","                                                embedded_sentence_1, embedded_sentence_2, embedded_sentence_3,\n","                                                embedded_sentence_4), 0)\n","    \n","    return claim_tensor, claim_claimant_tensor, claim_claimant_sentences_tensor, label_tensor, claim, claimant, sentence_0,\\\n","            [], label, id"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Collecting bpemb\n","  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb) (2.21.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bpemb) (3.6.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpemb) (4.28.1)\n","Collecting sentencepiece (from bpemb)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 7.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb) (1.16.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2019.6.16)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (3.0.4)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.3.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.8.4)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.12.0)\n","Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (2.49.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (1.9.189)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.2.1)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.9.4)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (1.12.189)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart-open>=1.2.1->gensim->bpemb) (2.5.3)\n","Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->smart-open>=1.2.1->gensim->bpemb) (0.14)\n","Installing collected packages: sentencepiece, bpemb\n","Successfully installed bpemb-0.3.0 sentencepiece-0.1.82\n","downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400869/400869 [00:00<00:00, 553702.89B/s]\n"],"name":"stderr"},{"output_type":"stream","text":["downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d50.w2v.bin.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1924908/1924908 [00:01<00:00, 1915305.23B/s]\n","/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gtyMhq16xyis","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n","        super().__init__() \n","        # We set d_ff as a default to 2048\n","        self.linear_1 = nn.Linear(d_model, d_ff)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear_2 = nn.Linear(d_ff, d_model)\n","    def forward(self, x):\n","        x = self.dropout(nn.relu(self.linear_1(x)))\n","        x = self.linear_2(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdjfFzOxx9E0","colab_type":"code","colab":{}},"source":["class Norm(nn.Module):\n","    def __init__(self, d_model, eps = 1e-6):\n","        super().__init__()\n","    \n","        self.size = d_model\n","        # create two learnable parameters to calibrate normalisation\n","        self.alpha = nn.Parameter(torch.ones(self.size))\n","        self.bias = nn.Parameter(torch.zeros(self.size))\n","        self.eps = eps\n","    def forward(self, x):\n","        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n","        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n","        return norm"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R2qiAJZTXYh-","colab_type":"text"},"source":["# Embedder"]},{"cell_type":"code","metadata":{"id":"-JDACacUuiB-","colab_type":"code","colab":{}},"source":["class Embedder(nn.Module):\n","    def __init__(self, vocab_size, d_model):\n","        super().__init__()\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","    def forward(self, x):\n","        return self.embed(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cCecZVBFvR5b","colab_type":"code","colab":{}},"source":["class PositionalEncoder(nn.Module):\n","    def __init__(self, d_model, max_seq_len = 80):\n","        super().__init__()\n","        self.d_model = d_model\n","        \n","        # create constant 'pe' matrix with values dependant on \n","        # pos and i\n","        pe = torch.zeros(max_seq_len, d_model)\n","        for pos in range(max_seq_len):\n","            for i in range(0, d_model, 2):\n","                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n","                \n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n"," \n","    \n","    def forward(self, x):\n","        # make embeddings relatively larger\n","        x = x * math.sqrt(self.d_model)\n","        #add constant to embedding\n","        seq_len = x.size(1)\n","        x = x + Variable(self.pe[:,:seq_len], \\\n","        requires_grad=False).cuda()\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u_XYgzCtXfBp","colab_type":"text"},"source":["# Multi-Headed Attention"]},{"cell_type":"code","metadata":{"id":"p_86rGBlwXZr","colab_type":"code","colab":{}},"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, heads, d_model, dropout = 0.1):\n","        super().__init__()\n","        \n","        self.d_model = d_model\n","        self.d_k = d_model // heads\n","        self.h = heads\n","        \n","        self.q_linear = nn.Linear(d_model, d_model)\n","        self.v_linear = nn.Linear(d_model, d_model)\n","        self.k_linear = nn.Linear(d_model, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        self.out = nn.Linear(d_model, d_model)\n","    \n","    def forward(self, q, k, v, mask=None):\n","        \n","        bs = q.size(0)\n","        \n","        # perform linear operation and split into h heads\n","        \n","        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n","        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n","        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n","        \n","        # transpose to get dimensions bs * h * sl * d_model\n","       \n","        k = k.transpose(1,2)\n","        q = q.transpose(1,2)\n","        v = v.transpose(1,2)\n","# calculate attention using function we will define next\n","        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n","        \n","        # concatenate heads and put through final linear layer\n","        concat = scores.transpose(1,2).contiguous()\\\n","        .view(bs, -1, self.d_model)\n","        \n","        output = self.out(concat)\n","    \n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZEmRJer1yJDT","colab_type":"code","colab":{}},"source":["# build an encoder layer with one multi-head attention layer and one # feed-forward layer\n","class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, heads, dropout = 0.1):\n","        super().__init__()\n","        self.norm_1 = Norm(d_model)\n","        self.norm_2 = Norm(d_model)\n","        self.attn = MultiHeadAttention(heads, d_model)\n","        self.ff = FeedForward(d_model)\n","        self.dropout_1 = nn.Dropout(dropout)\n","        self.dropout_2 = nn.Dropout(dropout)\n","        \n","    def forward(self, x, mask):\n","        x2 = self.norm_1(x)\n","        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n","        x2 = self.norm_2(x)\n","        x = x + self.dropout_2(self.ff(x2))\n","        return x\n","    \n","# We can then build a convenient cloning function that can generate multiple layers:\n","def get_clones(module, N):\n","    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Us6ds3IfyMMx","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    def __init__(self, vocab_size, d_model, N, heads):\n","        super().__init__()\n","        self.N = N\n","        self.embed = Embedder(vocab_size, d_model)\n","        self.pe = PositionalEncoder(d_model)\n","        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n","        self.norm = Norm(d_model)\n","    def forward(self, src):\n","        x = self.embed(src)\n","        x = self.pe(x)\n","#         for i in range(N):\n","#             x = self.layers[i](x, mask)\n","        return self.norm(x)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nptbbxYNaEFI","colab_type":"code","colab":{}},"source":["class Transformer(nn.Module):\n","    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n","        super().__init__()\n","        self.encoder = Encoder(src_vocab, d_model, N, heads)\n","        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n","        self.out = nn.Linear(d_model, trg_vocab)\n","    def forward(self, src, trg, src_mask, trg_mask):\n","        e_outputs = self.encoder(src, src_mask)\n","        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n","        output = self.out(d_output)\n","        return output\n","# we don't perform softmax on the output as this will be handled \n","# automatically by our loss function"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zaCP9j_bn8Ml","colab_type":"text"},"source":["# Training procedure"]},{"cell_type":"code","metadata":{"id":"fC-7W0IRn5-J","colab_type":"code","colab":{}},"source":["\n","\n","def train(category_tensor, line_tensor, update=True):\n","    rnnOptimizer.zero_grad()\n","    classifierOptimizer.zero_grad()\n","\n","    hidden = rnn.initHidden()\n","    output = model(line_tensor)\n","\n","    loss = criterion(output, category_tensor)\n","    if update:\n","        loss.backward()\n","        rnnOptimizer.step()\n","        classifierOptimizer.step()\n","        \n","    return output, loss.item()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bN9fUULKyVJJ","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"YVM5Oo0LyUSi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":253},"outputId":"41f29daf-c54e-486c-ed8c-6d918563b1f9","executionInfo":{"status":"error","timestamp":1564376359263,"user_tz":240,"elapsed":918572,"user":{"displayName":"Ming Huang","photoUrl":"https://lh5.googleusercontent.com/-zrotzI3mEl0/AAAAAAAAAAI/AAAAAAAAABU/J_3s5_fwd8U/s64/photo.jpg","userId":"07501724645488894229"}}},"source":["!pip install vocab\n","import vocab\n","import spacy\n","import torchtext\n","from torchtext.data import Field, BucketIterator, TabularDataset\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","europarl_en = open('/europarl-v7.fr-en.en', encoding='utf-8').read().split('\\n') # remember to upload each time\n","# europarl_fr = open('/europarl-v7.fr-en.fr', encoding='utf-8').read().split('\\n')\n","raw_data = {'English' : [line for line in europarl_en]}\n","df = pd.DataFrame(raw_data, columns=[\"English\"])\n","# remove very long sentences and sentences where translations are \n","# not of roughly equal length\n","df['eng_len'] = df['English'].str.count(' ')\n","# df['fr_len'] = df['French'].str.count(' ')\n","# df = df.query('fr_len < 80 & eng_len < 80')\n","# df = df.query('fr_len < eng_len * 1.5 & fr_len * 1.5 > eng_len')\n","\n","en = spacy.load('en')\n","def tokenize_en(sentence):\n","    return [tok.text for tok in en.tokenizer(sentence)]\n","train, val = train_test_split(df, test_size=0.1)\n","train.to_csv(\"train.csv\", index=False)\n","val.to_csv(\"val.csv\", index=False)\n","# associate the text in the 'English' column with the EN_TEXT field, # and 'French' with FR_TEXT\n","data_fields = [('English', EN_TEXT)]\n","train,val = TabularDataset.splits(path='./', train='train.csv', validation='val.csv', format='csv', fields=data_fields)\n","EN_TEXT.build_vocab(train, val)\n","\n","EN_TEXT = Field(tokenize=tokenize_en)\n","\n","d_model = 512\n","heads = 8\n","N = 6\n","train_iter = 1000\n","\n","src_vocab = len(EN_TEXT.vocab)\n","model = Encoder(src_vocab, d_model, N, heads)\n","\n","\n","for p in model.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","# this code is very important! It initialises the parameters with a\n","# range of values that stops the signal fading or getting too big.\n","# See this blog for a mathematical explanation.\n","\n","optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n","\n","\n","def train_model(epochs, print_every=100):\n","    \n","    model.train()\n","    \n","    start = time.time()\n","    temp = start\n","    \n","    total_loss = 0\n","    \n","    for epoch in range(epochs):\n","       \n","        for iter in range(1, train_iter + 1):\n","            src = batch.English.transpose(0,1)\n","            \n","            train_claim_tensor, train_claim_claimant_tensor, train_claim_claimant_sentences_tensor,\\\n","            label_tensor, claim_train, claimant, sentence_0,\\\n","            sentences, label_train, id = sampleClaim(train_data)\n","            if (id == \"SKIP\"): continue\n","\n","            # just claim\n","            train_output, train_loss = train(label_tensor, train_claim_tensor)    \n","            top_train_value, top_train_index = train_output.topk(1)\n","            train_guess_category = top_train_index[0].item()\n","            train_cumulative_loss += train_loss\n","            train_accuracy = 1 if train_guess_category == label_train else 0\n","            average_train_accuracy = (average_train_accuracy * count + train_accuracy) / (count+1)\n","\n","            # claim and claimant\n","            train_output_claim_claimant, train_loss_claim_claimant = train(label_tensor, train_claim_claimant_tensor)\n","            top_train_value_claim_claimant, top_train_index_claim_claimant = train_output_claim_claimant.topk(1)\n","            train_guess_category_claim_claimant = top_train_index_claim_claimant[0].item()\n","            train_cumulative_loss_claim_claimant += train_loss_claim_claimant\n","            train_accuracy_claim_claimant = 1 if train_guess_category_claim_claimant == label_train else 0\n","            average_train_accuracy_claim_claimant = (average_train_accuracy_claim_claimant * count + train_accuracy_claim_claimant) / (count+1)\n","\n","            # claim and claimant and sentences\n","            train_output_claim_claimant_sentences, train_loss_claim_claimant_sentences = train(label_tensor, train_claim_claimant_sentences_tensor)\n","            top_train_value_claim_claimant_sentences, top_train_index_claim_claimant_sentences = train_output_claim_claimant_sentences.topk(1)\n","            train_guess_category_claim_claimant_sentences = top_train_index_claim_claimant_sentences[0].item()\n","            train_cumulative_loss_claim_claimant_sentences += train_loss_claim_claimant_sentences\n","            train_accuracy_claim_claimant_sentences = 1 if train_guess_category_claim_claimant_sentences == label_train else 0\n","            average_train_accuracy_claim_claimant_sentences = (average_train_accuracy_claim_claimant_sentences * count + train_accuracy_claim_claimant) / (count+1)\n","\n","            # separate train and test line\n","            test_claim_tensor, test_claim_claimant_tensor, test_claim_claimant_sentences_tensor,\\\n","            label_tensor, claim_test, claimant, sentence_0,\\\n","            sentences, label_test, id = sampleClaim(test_data)\n","            if (id == \"SKIP\"): continue\n","\n","            # just claim\n","            test_output, test_loss = train(label_tensor, test_claim_tensor, update=False)\n","            top_test_value, top_test_index = test_output.topk(1)\n","            test_guess_category = top_test_index[0].item()\n","            test_cumulative_loss += test_loss\n","            test_accuracy = 1 if test_guess_category == label_test else 0\n","            average_test_accuracy = (average_test_accuracy * count + test_accuracy) / (count+1)\n","\n","            # claim and claimant\n","            test_output_claim_claimant, test_loss_claim_claimant = train(label_tensor, test_claim_claimant_tensor, update=False)\n","            top_test_value_claim_claimant, top_test_index_claim_claimant = test_output_claim_claimant.topk(1)\n","            test_guess_category_claim_claimant = top_test_index_claim_claimant[0].item()\n","            test_cumulative_loss_claim_claimant += test_loss_claim_claimant\n","            test_accuracy_claim_claimant = 1 if test_guess_category_claim_claimant == label_test else 0\n","            average_test_accuracy_claim_claimant = (average_test_accuracy_claim_claimant * count + test_accuracy_claim_claimant) / (count+1)\n","\n","            # claim and claimant and sentences\n","            test_output_claim_claimant_sentences, test_loss_claim_claimant_sentences = train(label_tensor, test_claim_claimant_sentences_tensor, update=False)\n","            top_test_value_claim_claimant_sentences, top_test_index_claim_claimant_sentences = test_output_claim_claimant_sentences.topk(1)\n","            test_guess_category_claim_claimant_sentences = top_test_index_claim_claimant_sentences[0].item()\n","            test_cumulative_loss_claim_claimant_sentences += test_loss_claim_claimant_sentences\n","            test_accuracy_claim_claimant_sentences = 1 if test_guess_category_claim_claimant_sentences == label_test else 0\n","            average_test_accuracy_claim_claimant_sentences = (average_test_accuracy_claim_claimant_sentences * count + test_accuracy_claim_claimant_sentences) / (count+1)\n","            count += 1\n","            \n","            # Add current loss avg to list of losses\n","            if iter % plot_every == 0:\n","                train_correct = '✓' if train_guess_category == label else '✗ (%s)' % label\n","                print('Train: %d  %d%% (%s) average_accuracy=%.4f average_loss=%.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), average_train_accuracy, train_cumulative_loss / plot_every, claim_train, train_guess_category, train_correct))\n","                test_correct = '✓' if test_guess_category == label else '✗ (%s)' % label\n","                print('Test: %d  %d%% (%s) average_accuracy=%.4f average_loss=%.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), average_test_accuracy, test_cumulative_loss / plot_every, claim_test, test_guess_category, test_correct))\n","                all_train_losses.append(train_cumulative_loss / plot_every)\n","                all_train_accuracies.append(average_train_accuracy)\n","                all_test_losses.append(test_cumulative_loss / plot_every)\n","                all_test_accuracies.append(average_test_accuracy)\n","\n","                all_train_losses_claim_claimant.append(train_cumulative_loss_claim_claimant / plot_every)\n","                all_train_accuracies_claim_claimant.append(average_train_accuracy_claim_claimant)\n","                all_test_losses_claim_claimant.append(test_cumulative_loss_claim_claimant / plot_every)\n","                all_test_accuracies_claim_claimant.append(average_test_accuracy_claim_claimant)\n","\n","                all_train_losses_claim_claimant_sentences.append(train_cumulative_loss_claim_claimant_sentences / plot_every)\n","                all_train_accuracies_claim_claimant_sentences.append(average_train_accuracy_claim_claimant_sentences)\n","                all_test_losses_claim_claimant_sentences.append(test_cumulative_loss_claim_claimant_sentences / plot_every)\n","                all_test_accuracies_claim_claimant_sentences.append(average_test_accuracy_claim_claimant_sentences)\n","\n","                train_cumulative_loss = 0\n","                average_train_accuracy = 0\n","                test_cumulative_loss = 0\n","                average_test_accuracy = 0\n","\n","                train_cumulative_loss_claim_claimant = 0\n","                average_train_accuracy_claim_claimant = 0\n","                test_cumulative_loss_claim_claimant = 0\n","                average_test_accuracy_claim_claimant = 0\n","\n","                train_cumulative_loss_claim_claimant_sentences = 0\n","                average_train_accuracy_claim_claimant_sentences = 0\n","                test_cumulative_loss_claim_claimant_sentences = 0\n","                average_test_accuracy_claim_claimant_sentences = 0\n","                count = 0\n"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: vocab in /usr/local/lib/python3.6/dist-packages (0.0.4)\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-8f071dbcd637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0msrc_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEN_TEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Field' object has no attribute 'vocab'"]}]},{"cell_type":"code","metadata":{"id":"xHA5JYbFu1Ue","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","f1 = plt.figure()\n","ax1 = f1.add_subplot(111)\n","ax1.set_title('Training Accuracy')\n","ax1.plot(all_train_accuracies)  # i, ii, iii accuracies \n","ax1.plot(all_train_accuracies_claim_claimant) \n","ax1.plot(all_train_accuracies_claim_claimant_sentences) \n","ax1.set_ylabel('Accuracy')\n","ax1.set_xlabel('Epoch')\n","ax1.legend(['Claim only', 'Claim and claimant', 'Claim, the claimant and the 5 sentences'], loc='upper left')\n","f1.savefig(\"q2b_train.png\")\n","\n","f2 = plt.figure()\n","ax2 = f2.add_subplot(111)\n","ax2.set_title('Test Accuracy')\n","ax2.plot(all_test_accuracies) \n","ax2.plot(all_test_accuracies_claim_claimant) \n","ax2.plot(all_test_accuracies_claim_claimant_sentences) \n","ax2.set_ylabel('Accuracy')\n","ax2.set_xlabel('Epoch')\n","ax2.legend(['Claim only', 'Claim and claimant', 'Claim, the claimant and the 5 sentences'], loc='upper left')\n","f2.savefig(\"q2b_test.png\")"],"execution_count":0,"outputs":[]}]}