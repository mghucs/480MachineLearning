{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs480_leadersprize.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D-1skF0AuPV",
        "colab_type": "text"
      },
      "source": [
        "# Load metadata\n",
        "\n",
        "This assumes that the file train.zip has been unzipped in the current directory.  If needed, insert code here to load the data from your computer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOSRAu7VAl29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# load metadata\n",
        "with open(\"train.json\", 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "n_claims = len(metadata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBMQ2trzpJz3",
        "colab_type": "text"
      },
      "source": [
        "# RNN architecture\n",
        "\n",
        "We define a simple RNN that processes one token at time to update a hidden vector.  The last hidden vector is passed to a classifier that uses a softmax to predict the category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6BH8Bgpa_ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        return hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "      \n",
        "      \n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, hidden):\n",
        "        hidden = self.h2o(hidden)\n",
        "        output = self.softmax(hidden)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlGjMjGLqwe1",
        "colab_type": "text"
      },
      "source": [
        "# Sample a claim\n",
        "Sample a claim, tokenize it and embed it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRm_PGtUp2TM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bpemb\n",
        "from bpemb import BPEmb\n",
        "n_embedding_dims = 50\n",
        "bpemb_en = BPEmb(lang=\"en\", dim=n_embedding_dims)\n",
        "\n",
        "def sampleClaim(metadata): \n",
        "    id = random.randint(0, len(metadata) - 1)\n",
        "    claim = metadata[id][\"claim\"]\n",
        "    embedding = bpemb_en.embed(claim)\n",
        "    embedding = np.reshape(embedding,(embedding.shape[0],1,embedding.shape[1]))\n",
        "    label = metadata[id][\"label\"]\n",
        "    label_tensor = torch.tensor([label], dtype=torch.long)\n",
        "    claim_tensor = torch.tensor(embedding, dtype=torch.float)\n",
        "    return claim_tensor, label_tensor, claim, label, id\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvyIR3hWrcAt",
        "colab_type": "text"
      },
      "source": [
        "# Training procedure\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwNG-K1srY0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(category_tensor, line_tensor, update=True):\n",
        "    rnnOptimizer.zero_grad()\n",
        "    classifierOptimizer.zero_grad()\n",
        "\n",
        "    hidden = rnn.initHidden()\n",
        "    for i in range(line_tensor.size()[0]):\n",
        "        hidden = rnn(line_tensor[i], hidden)\n",
        "    output = classifier(hidden)\n",
        "\n",
        "    loss = criterion(output, category_tensor)\n",
        "    if update:\n",
        "        loss.backward()\n",
        "        rnnOptimizer.step()\n",
        "        classifierOptimizer.step()\n",
        "        \n",
        "    return output, loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drxAgcTdrxkB",
        "colab_type": "text"
      },
      "source": [
        "# Train RNN and plot results\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJs19dDZrwOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "n_hidden = 128\n",
        "n_categories = 3\n",
        "rnn = RNN(n_embedding_dims, n_hidden)\n",
        "classifier = Classifier(n_hidden, n_categories)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "learning_rate = 1e-4\n",
        "rnnOptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "classifierOptimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "n_iters = 100000\n",
        "plot_every = 1000\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "train_data = metadata[:10000]\n",
        "test_data = metadata[10000:]\n",
        "train_cumulative_loss = 0\n",
        "test_cumulative_loss = 0\n",
        "average_train_accuracy = 0\n",
        "average_test_accuracy = 0\n",
        "all_train_losses = []\n",
        "all_train_accuracies = []\n",
        "all_test_losses = []\n",
        "all_test_accuracies = []\n",
        "count = 0\n",
        "for iter in range(1, n_iters + 1):\n",
        "    train_line_tensor, train_category_tensor, train_line, train_category, train_id = sampleClaim(train_data)\n",
        "    train_output, train_loss = train(train_category_tensor, train_line_tensor)    \n",
        "    top_train_value, top_train_index = train_output.topk(1)\n",
        "    train_guess_category = top_train_index[0].item()\n",
        "    train_cumulative_loss += train_loss\n",
        "    train_accuracy = 1 if train_guess_category == train_category else 0\n",
        "    average_train_accuracy = (average_train_accuracy * count + train_accuracy) / (count+1)\n",
        "    \n",
        "    test_line_tensor, test_category_tensor, test_line, test_category, test_id = sampleClaim(test_data)\n",
        "    test_output, test_loss = train(test_category_tensor, test_line_tensor, update=False)\n",
        "    top_test_value, top_test_index = test_output.topk(1)\n",
        "    test_guess_category = top_test_index[0].item()\n",
        "    test_cumulative_loss += test_loss\n",
        "    test_accuracy = 1 if test_guess_category == test_category else 0\n",
        "    average_test_accuracy = (average_test_accuracy * count + test_accuracy) / (count+1)\n",
        "    count += 1\n",
        "\n",
        "    # Add current loss avg to list of losses\n",
        "    if iter % plot_every == 0:\n",
        "        train_correct = '✓' if train_guess_category == train_category else '✗ (%s)' % train_category\n",
        "        print('Train: %d  %d%% (%s) average_accuracy=%.4f average_loss=%.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), average_train_accuracy, train_cumulative_loss / plot_every, train_line, train_guess_category, train_correct))\n",
        "        test_correct = '✓' if test_guess_category == test_category else '✗ (%s)' % test_category\n",
        "        print('Test: %d  %d%% (%s) average_accuracy=%.4f average_loss=%.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), average_test_accuracy, test_cumulative_loss / plot_every, test_line, test_guess_category, test_correct))\n",
        "        all_train_losses.append(train_cumulative_loss / plot_every)\n",
        "        all_train_accuracies.append(average_train_accuracy)\n",
        "        all_test_losses.append(test_cumulative_loss / plot_every)\n",
        "        all_test_accuracies.append(average_test_accuracy)\n",
        "        train_cumulative_loss = 0\n",
        "        average_train_accuracy = 0\n",
        "        test_cumulative_loss = 0\n",
        "        average_test_accuracy = 0\n",
        "        count = 0\n",
        "        \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_train_accuracies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AFpnb5eoXRn",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess articles\n",
        "\n",
        "This code preprocesses the aticles to extract the top 5 sentences with greatest similarity to the claim according to tf-idf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAUJbu7Sv4ey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_articles():\n",
        "\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    \n",
        "    # load metadata\n",
        "    with open(\"train.json\", 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "    n_claims = len(metadata)\n",
        "\n",
        "    # load related articles for each claim\n",
        "    relevant_sentences = []\n",
        "    for id in range(n_claims):\n",
        "  \n",
        "        if id % 500 == 0:\n",
        "            print(\"Claims preprocessed: \",id)\n",
        "        \n",
        "        # retrieve related articles\n",
        "        related_articles = metadata[id]['related_articles']\n",
        "        articles = \"\"\n",
        "        for article_id in related_articles:\n",
        "            filename = \"train_articles/\" + str(article_id) + \".txt\"\n",
        "            # concatenate related articles\n",
        "            with open(filename, 'r') as text_file:\n",
        "                text = text_file.read()\n",
        "                articles = articles + \"\\n\" + text\n",
        "\n",
        "        # split articles into sentences\n",
        "        sentences = sent_tokenize(articles)\n",
        "\n",
        "        # append claim to articles\n",
        "        sentences.append(metadata[id]['claim'])\n",
        "\n",
        "        # vectorize sentences based on tf-idf\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        X = vectorizer.fit_transform(sentences)\n",
        "    \n",
        "        # measure similarity between claim and each sentence\n",
        "        similarity =  X[-1,:] @ np.transpose(X[:-2,:])\n",
        "        similarity = similarity.todense()\n",
        "\n",
        "        # find top 5 sentences with greatest similarity\n",
        "        sorted_index = np.argsort(similarity)\n",
        "        top_sentences = []\n",
        "        for i in range(1,min(5,sorted_index.shape[1])+1):\n",
        "            top_sentences.append(sentences[sorted_index[0,-i]])\n",
        "        relevant_sentences.append(top_sentences)\n",
        "\n",
        "   \n",
        "    return metadata, relevant_sentences\n",
        "\n",
        "metadata, relevant_sentences = preprocess_articles()\n",
        "print(\"Metadata of claim 0:\")\n",
        "print(metadata[0]['claim'])\n",
        "print(\"Relevant sentences of claim 0:\")\n",
        "print(relevant_sentences[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}